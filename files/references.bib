

@article{BaroniCommAIEvaluatingfirst2017,
  title = {{{CommAI}}: {{Evaluating}} the First Steps towards a Useful General {{AI}}},
  shorttitle = {{{CommAI}}},
  journaltitle = {arXiv preprint arXiv:1701.08954},
  author = {Baroni, Marco and Joulin, Armand and Jabri, Allan and Kruszewski, GermÃ n and Lazaridou, Angeliki and Simonic, Klemen and Mikolov, Tomas},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/BXHKL6H6/baroni2017commai.pdf}
}

@article{HillUnderstandingGroundedLanguage2017,
  title = {Understanding {{Grounded Language Learning Agents}}},
  journaltitle = {arXiv preprint arXiv:1710.09867},
  author = {Hill, Felix and Hermann, Karl Moritz and Blunsom, Phil and Clark, Stephen},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/N85SIG8S/hill2017understanding.pdf}
}

@inproceedings{LoweAutomaticTuringTest2017,
  langid = {english},
  title = {Towards an {{Automatic Turing Test}}: {{Learning}} to {{Evaluate Dialogue Responses}}},
  url = {http://aclweb.org/anthology/P17-1103},
  doi = {10.18653/v1/P17-1103},
  shorttitle = {Towards an {{Automatic Turing Test}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Lowe, Ryan and Noseworthy, Michael and Serban, Iulian Vlad and Angelard-Gontier, Nicolas and Bengio, Yoshua and Pineau, Joelle},
  urldate = {2017-11-17},
  date = {2017},
  pages = {1116--1126},
  file = {/home/theo/Zotero/storage/FGAC8MH3/lowe2017towards.pdf}
}

@article{BeulsAgentBasedModelsStrategies2013,
  langid = {english},
  title = {Agent-{{Based Models}} of {{Strategies}} for the {{Emergence}} and {{Evolution}} of {{Grammatical Agreement}}},
  volume = {8},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0058960},
  doi = {10.1371/journal.pone.0058960},
  number = {3},
  journaltitle = {PLoS ONE},
  author = {Beuls, Katrien and Steels, Luc},
  editor = {SolÃ©, Ricard V.},
  urldate = {2017-11-17},
  date = {2013-03-18},
  pages = {e58960},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/8CE3IKY3/beuls2013agent-based.PDF}
}

@book{SteelsTalkingHeadsexperiment2015,
  langid = {english},
  location = {{Berlin}},
  title = {The {{Talking Heads}} Experiment: Origins of Words and Meanings},
  isbn = {978-3-944675-42-8 978-3-944675-76-3 978-3-944675-77-0},
  shorttitle = {The {{Talking Heads}} Experiment},
  pagetotal = {375},
  number = {1},
  series = {Computational models of language evolution},
  publisher = {{Language Science Press}},
  author = {Steels, Luc},
  date = {2015},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/FGMGSG4X/steels2015the_talking.pdf},
  note = {OCLC: 927146551}
}

@article{LazaridouMultiagentcooperationemergence2016,
  title = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
  journaltitle = {arXiv preprint arXiv:1612.07182},
  author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
  date = {2016},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/YIX9SQHH/lazaridou2016multi-agent.pdf}
}

@article{LewisDealnodeal2017,
  title = {Deal or No Deal? End-to-End Learning for Negotiation Dialogues},
  shorttitle = {Deal or No Deal?},
  journaltitle = {arXiv preprint arXiv:1706.05125},
  author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/CRN2D7AM/lewis2017deal_or_no_deal.pdf}
}

@article{MordatchEmergenceGroundedCompositional2017,
  title = {Emergence of {{Grounded Compositional Language}} in {{Multi}}-{{Agent Populations}}},
  journaltitle = {arXiv preprint arXiv:1703.04908},
  author = {Mordatch, Igor and Abbeel, Pieter},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/SYA73ZXY/mordatch2017emergence_of.pdf}
}

@article{LeeEmergenttranslationmultiagent2017,
  title = {Emergent Translation in Multi-Agent Communication},
  journaltitle = {arXiv preprint arXiv:1710.06922},
  author = {Lee, Jason and Cho, Kyunghyun and Weston, Jason and Kiela, Douwe},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/VP8D8VKW/lee2017emergent.pdf}
}

@article{Serbandeepreinforcementlearning2017,
  title = {A Deep Reinforcement Learning Chatbot},
  journaltitle = {arXiv preprint arXiv:1709.02349},
  author = {Serban, Iulian V. and Sankar, Chinnadhurai and Germain, Mathieu and Zhang, Saizheng and Lin, Zhouhan and Subramanian, Sandeep and Kim, Taesup and Pieper, Michael and Chandar, Sarath and Ke, Nan Rosemary},
  date = {2017},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/FJ868G2T/serban2017a_deep.pdf}
}

@article{HenaffTrackingWorldState2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.03969},
  primaryClass = {cs},
  title = {Tracking the {{World State}} with {{Recurrent Entity Networks}}},
  url = {http://arxiv.org/abs/1612.03969},
  abstract = {We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.},
  author = {Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
  urldate = {2017-11-22},
  date = {2016-12-12},
  keywords = {Computer Science - Computation and Language,_tablet},
  file = {/home/theo/Zotero/storage/DIIHJBSQ/henaff2016tracking_the.pdf;/home/theo/Zotero/storage/6JF8XNLA/1612.html}
}

@article{BordesLearningEndtoEndGoalOriented2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07683},
  primaryClass = {cs},
  title = {Learning {{End}}-to-{{End Goal}}-{{Oriented Dialog}}},
  url = {http://arxiv.org/abs/1605.07683},
  abstract = {Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.},
  author = {Bordes, Antoine and Boureau, Y.-Lan and Weston, Jason},
  urldate = {2017-11-22},
  date = {2016-05-24},
  keywords = {Computer Science - Computation and Language},
  file = {/home/theo/Zotero/storage/6SD5H7V8/Bordes et al. - 2016 - Learning End-to-End Goal-Oriented Dialog.pdf;/home/theo/Zotero/storage/9TX5FHBW/1605.html}
}

@article{SukhbaatarLearningMultiagentCommunication2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07736},
  primaryClass = {cs},
  title = {Learning {{Multiagent Communication}} with {{Backpropagation}}},
  url = {http://arxiv.org/abs/1605.07736},
  abstract = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
  author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob},
  urldate = {2017-11-26},
  date = {2016-05-25},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,_tablet},
  file = {/home/theo/Zotero/storage/GDMWXCCS/sukhbaatar2016learning.pdf;/home/theo/Zotero/storage/SVCY3QF3/1605.html}
}

@article{FoersterLearningCommunicateSolve2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02672},
  primaryClass = {cs},
  title = {Learning to {{Communicate}} to {{Solve Riddles}} with {{Deep Distributed Recurrent Q}}-{{Networks}}},
  url = {http://arxiv.org/abs/1602.02672},
  abstract = {We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.},
  author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
  urldate = {2017-11-26},
  date = {2016-02-08},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,_tablet},
  options = {useprefix=true},
  file = {/home/theo/Zotero/storage/E45TLFDQ/foerster2016learning_to.pdf;/home/theo/Zotero/storage/RMG7I835/1602.html}
}

@article{GravesHumanlevelcontroldeep2015,
  langid = {english},
  title = {Human-Level Control through Deep Reinforcement Learning},
  volume = {518},
  rights = {2015 Nature Publishing Group},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature14236},
  doi = {10.1038/nature14236},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.$<$/p$>$},
  number = {7540},
  journaltitle = {Nature},
  author = {Graves, Alex and Sadik, Amir and Fidjeland, Andreas K. and Rusu, Andrei A. and Beattie, Charles and Wierstra, Daan and Silver, David and Hassabis, Demis and Kumaran, Dharshan and Ostrovski, Georg and King, Helen and Antonoglou, Ioannis and Veness, Joel and Kavukcuoglu, Koray and Bellemare, Marc G. and Riedmiller, Martin and Legg, Shane and Petersen, Stig and Mnih, Volodymyr},
  urldate = {2017-11-26},
  date = {2015-02-25},
  pages = {529},
  keywords = {_tablet},
  file = {/home/theo/Zotero/storage/78553WZX/graves2015human-level.pdf;/home/theo/Zotero/storage/PQ69AMAK/nature14236.html}
}


@inproceedings{faruqui2015retrofitting,
  title={Retrofitting Word Vectors to Semantic Lexicons},
  author={Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
  booktitle={Proceedings of NAACL},
  year={2015}
}

@inproceedings{faruqui2015sparse,
  title={Sparse overcomplete word vector representations},
  author={Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah},
  booktitle={Proceedings of ACL},
  year={2015}
}

@article{hill2016simlex,
  title={Simlex-999: Evaluating semantic models with (genuine) similarity estimation},
  author={Hill, Felix and Reichart, Roi and Korhonen, Anna},
  journal={Computational Linguistics},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{bartunov2016breaking,
  title={Breaking sticks and ambiguities with adaptive skip-gram},
  author={Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  booktitle={Artificial Intelligence and Statistics},
  pages={130--138},
  year={2016}
}

@article{nickel2017poincar,
  title={Poincaré Embeddings for Learning Hierarchical Representations},
  author={Nickel, Maximilian and Kiela, Douwe},
  journal={arXiv preprint arXiv:1705.08039},
  year={2017}
}

@article{levy2015improving,
  title={Improving distributional similarity with lessons learned from word embeddings},
  author={Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={211--225},
  year={2015}
}

@article{arora2016linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1601.03764},
  year={2016}
}

@article{arora2016latent,
  title={A latent variable model approach to pmi-based word embeddings},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={385--399},
  year={2016}
}

@article{faruqui2015non,
  title={Non-distributional word vector representations},
  author={Faruqui, Manaal and Dyer, Chris},
  journal={arXiv preprint arXiv:1506.05230},
  year={2015}
}

@inproceedings{gouws2015bilbowa,
  title={Bilbowa: Fast bilingual distributed representations without word alignments},
  author={Gouws, Stephan and Bengio, Yoshua and Corrado, Greg},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={748--756},
  year={2015}
}

@article{tsvetkov2015evaluation,
  title={Evaluation of word vector representations by subspace alignment},
  author={Tsvetkov, Yulia and Faruqui, Manaal and Ling, Wang and Lample, Guillaume and Dyer, Chris},
  year={2015},
  publisher={Citeseer}
}

@inproceedings{liu2015learning,
  title={Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints.},
  author={Liu, Quan and Jiang, Hui and Wei, Si and Ling, Zhen-Hua and Hu, Yu},
  booktitle={ACL (1)},
  pages={1501--1511},
  year={2015}
}

@inproceedings{gladkova2016intrinsic,
  title={Intrinsic Evaluations of Word Embeddings: What Can We Do Better?},
  author={Gladkova, Anna and Drozd, Aleksandr},
  booktitle={ACL 2016},
  pages={36},
  year={2016}
}

@inproceedings{drozd2016word,
  title={Word Embeddings, Analogies, and Machine Learning: Beyond king-man+ woman= queen.},
  author={Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
  booktitle={COLING},
  pages={3519--3530},
  year={2016}
}

@inproceedings{levy2014neural,
  title={Neural word embedding as implicit matrix factorization},
  author={Levy, Omer and Goldberg, Yoav},
  booktitle={Advances in neural information processing systems},
  pages={2177--2185},
  year={2014}
}

@InProceedings{ling2015two,
author = {Ling, Wang and Dyer, Chris and Black, Alan and Trancoso, Isabel},
title="Two/Too Simple Adaptations of word2vec for Syntax Problems",
booktitle="Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
year="2015",
publisher="Association for Computational Linguistics",
location="Denver, Colorado",
}

@article{mikolov2013exploiting,
  title={Exploiting similarities among languages for machine translation},
  author={Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1309.4168},
  year={2013}
}

@inproceedings{huang2012improving,
  title={Improving word representations via global context and multiple word prototypes},
  author={Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
  pages={873--882},
  year={2012},
  organization={Association for Computational Linguistics}
}

@article{neelakantan2015efficient,
  title={Efficient non-parametric estimation of multiple embeddings per word in vector space},
  author={Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
  journal={arXiv preprint arXiv:1504.06654},
  year={2015}
}

@inproceedings{levy2014linguistic,
  title={Linguistic Regularities in Sparse and Explicit Word Representations.},
  author={Levy, Omer and Goldberg, Yoav and Ramat-Gan, Israel},
  booktitle={CoNLL},
  pages={171--180},
  year={2014}
}

@article{hinton1984distributed,
  title={Distributed representations},
  author={Hinton, Geoffrey E},
  year={1984}
}

@article{arora2015random,
  title={Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1502.03520},
  year={2015}
}

@article{trask2015sense2vec,
  title={sense2vec-A fast and accurate method for word sense disambiguation in neural word embeddings},
  author={Trask, Andrew and Michalak, Phil and Liu, John},
  journal={arXiv preprint arXiv:1511.06388},
  year={2015}
}

@inproceedings{levy2014dependency,
  title={Dependency-Based Word Embeddings.},
  author={Levy, Omer and Goldberg, Yoav},
  booktitle={ACL (2)},
  pages={302--308},
  year={2014},
  organization={Citeseer}
}


@inproceedings{reisinger2010multi,
  title={Multi-prototype vector-space models of word meaning},
  author={Reisinger, Joseph and Mooney, Raymond J},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={109--117},
  year={2010},
  organization={Association for Computational Linguistics}
}

@article{avraham2016improving,
  title={Improving reliability of word similarity evaluation by redesigning annotation task and performance measure},
  author={Avraham, Oded and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1611.03641},
  year={2016}
}

@inproceedings{mnih2009scalable,
  title={A scalable hierarchical distributed language model},
  author={Mnih, Andriy and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1081--1088},
  year={2009}
}

@article{mnih2012fast,
  title={A fast and simple algorithm for training neural probabilistic language models},
  author={Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1206.6426},
  year={2012}
}

@article{goldberg2014word2vec,
  title={word2vec explained: Deriving mikolov et al.'s negative-sampling word-embedding method},
  author={Goldberg, Yoav and Levy, Omer},
  journal={arXiv preprint arXiv:1402.3722},
  year={2014}
}

@article{rong2014word2vec,
  title={word2vec parameter learning explained},
  author={Rong, Xin},
  journal={arXiv preprint arXiv:1411.2738},
  year={2014}
}

@inproceedings{pennington2014glove,
  title={Glove: Global Vectors for Word Representation.},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={EMNLP},
  volume={14},
  pages={1532--1543},
  year={2014}
}

@inproceedings{brosseau2010towards,
  title={Towards an optimal weighting of context words based on distance},
  author={Brosseau-Villeneuve, Bernard and Nie, Jian-Yun and Kando, Noriko},
  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics},
  pages={107--115},
  year={2010},
  organization={Association for Computational Linguistics}
}

@article{firth1957synopsis,
  title={A synopsis of linguistic theory, 1930-1955},
  author={Firth, John R},
  journal={Studies in linguistic analysis},
  year={1957},
  publisher={Basil Blackwell}
}

@inproceedings{baroni2014don,
  title={Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.},
  author={Baroni, Marco and Dinu, Georgiana and Kruszewski, Germ{\'a}n},
  booktitle={ACL (1)},
  pages={238--247},
  year={2014}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{hamilton2016diachronic,
  title={Diachronic word embeddings reveal statistical laws of semantic change},
  author={Hamilton, William L and Leskovec, Jure and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1605.09096},
  year={2016}
}
