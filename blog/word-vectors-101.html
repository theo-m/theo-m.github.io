<!DOCTYPE html>
<html>
<head>
  <title>Word Vectors 101</title>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/highlighter.css">
  <link rel=icon href=/favicon.png>
</head>

<body>
  <div class="container">
    <h1 class="title"> Word Vectors 101 <span style="color: #b6b6b6;font-size: small"> 2017-10-18 </span> </h1>
    <p>The following is yet another attempt at describing what word vectors are. Numerous blog posts have done it before, among which <a href="hisblog">sruder</a>, <a href="hisblog">acolyer</a> or <a href="hisblog">colah</a>, all of them being <em>very</em> good articles. I wrote the following since I believe what I have learnt during my internship at the <em>Université de Montréal</em> is valuable as it differs from the cited blog posts.</p>

<p>Word vectors were first thought of in the late 80s by Geoff Hinton <a class="citation" href="#hinton1984distributed">Hinton 1984</a> and they have been a topic of research ever since. What motivates them is simply the bold idea to translate words of our common vocabulary into mathematical vectors. This “translation” is hard to define as it is unclear what we actually want it to be; i.e. how we want the vectors to carry <em>meaning</em>, a notion sufficiently hard to properly define that it requires its own philosophic discipline.</p>

<p>The first and main idea would be that this translation — a mapping in mathematical terms and defined as such:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
	\phi :\quad & \mathcal V \longrightarrow \mathbb R^d \\ & w \mapsto \phi(w) 
	\end{align} %]]></script>

<p>be <em>continuous</em>, a mathematical term denoting the fact that we want two words close in meaning to be close in the vector space. Strictly put, this means that we expect the following:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{gathered}
	\forall \epsilon > 0 \exists \delta > 0 \quad s.t.\\
	d(w_1,w_2)<\delta \implies \Vert \phi(w_1) - \phi(w_2) \Vert_2^2 < \epsilon
	\end{gathered} %]]></script>

<p>Putting this clearly on paper makes us wonder about what exactly we expect $d$, the distance on our vocabulary, to be. While $\Vert\cdot\Vert_2^2$ is well defined as the euclidean norm for $\mathbb R^d$, it is unclear how to put a good metric on the vocabulary space. Should <code class="highlighter-rouge">plant</code> be close to <code class="highlighter-rouge">flower</code> or to <code class="highlighter-rouge">chemical</code>?</p>

<p>But such questions will be left for later inquiries; let us now see how to actually generate such vectors. In 2014, following great movements in the discipline, Baroni <a class="citation" href="#baroni2014don">Baroni, Dinu, and Kruszewski 2014</a> divided generation methods between count-based and predictive methods. This classification is based on the heuristic used to determine the <em>cooccurence information</em> of a pair of words.</p>

<p>…</p>

<hr />

<p><code class="highlighter-rouge"><span class="gd">--- 01/12/2017 update ---</span></code></p>

<p>This was an article I wanted to write after my internship in Montreal. It has stayed this long for 3 months now which is a probable sign that I won’t ever finish it. I am leaving this preamble however since in my opinion this idea of semantic continuity is original and not adressed enough.</p>

<p>Fortunately Sebastian Ruder does a much better work than I do and published an excellent article on his <a href="http://ruder.io/word-embeddings-2017/index.html">blog</a>.</p>

<p>My own bibliography can be found on this <a href="https://www.zotero.org/theo-m/items/collectionKey/G4TF65NM">Zotero collection</a>, it is complementary, but not annotated.</p>

<hr />

<p>Cited references</p>

<ol class="bibliography"><li><span id="hinton1984distributed">Hinton, Geoffrey E. 1984. “Distributed Representations.”</span></li>
<li><span id="baroni2014don">Baroni, Marco, Georgiana Dinu, and Germán Kruszewski. 2014. “Don’t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors.” In <i>ACL (1)</i>, 238–47.</span></li></ol>

  </div>
</body>

<footer><script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></footer>

</html>