---
layout: post
title: "word embeddings 1: an introduction"
date: 2017-06-29
---

![embeddings](/files/embeddings.png)

Word embeddings, _a.k.a._ vector space models for words, are a convenient way to represent words as vectors. Doing so has a lot of advantages, but the first and foremost motivation is to be able to apply all the theory on linear algebra to language.

The idea has been around for a _long_ time, at least since {% cite hinton1984distributed %}, but it remained a nice but computationally expensive idea up until {% cite mikolov2013efficient %}. 

The big idea is to learn from a corpus a certain idea of how words cooccur together. The notion of cooccurence is simple: we're interested in counting the number of times two words appear at distance less than \\( k \\), where usually \\(2 \lt k \lt 10\\), of each other.

The underlying idea to counting cooccurences is that of distributional semantics, which can be described with a famous quote:

{% quote firth1957synopsis%}
A word is characterized by the company it keeps.
{% endquote %}

There are many different ways to keep count of cooccurences, and many different ways to deal with those numbers to infuse meaning into word vectors.

Along with {% cite baroni2014don %}, we will oppose the count-based methods and the predictive methods. This opposition is not a strong one as we will see that both approaches have strong similarities.

### Count based: the old way

We start by converting words into numbers, this is usually done by ranking them according to word frequency, _i.e._ `'the'` is `1`, `'of'` is `2`, etc... and by lexicographical order for words of same frequency.

The vocabulary is often the words occuring more than \\( m \\) times in the corpus, plus an `UNK` token serving as a catch-all for rare words.  
Denoting \\(v\\) the size of the vocabulary, we can build a matrix

$$ M \in \mathbb{R}^{v\times v} $$

that we instantiate at 0. When we process the texts, if `word1` is appearing close to `word2`, we will simply do:

$$ M(i_{\text{word1}}, i_{\text{word2}}) \mathrel{+}= 1,~~ (i_{\text{word1}}, i_{\text{word2}}) \in \mathbb{N}^2 $$


Now we procede in a somewhat unoriginal manner for machine learning; we're going to do a dimensionality reduction on this matrix, via a [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition?oldformat=true). This process gives us a new basis to describe our matrix, and we're going to stick with the \\( d \\) vectors of the basis with the biggest associated singular values. Usually we take \\( 20 \lt d \lt 500 \\).

A subtler approach consists in taking not \\( M \\) as simply a count of cooccurences, but as a matrix of the _pointwise mutual information_ which is defined as the log ratio between two words' joint probability and the product of their marginal probabilities.

$$ M = \big[\text{PMI}(i, j)\big]_{0 < i,j < v} $$

$$ \text{PMI}(i,j) = \log \frac{\#(i,j) * \text{corpus size}}{\#(i)\#(j)} $$


This process, with all its possible tweaks and modifications, is qualified as **count-based** since we base the vectors upon computations of cooccurences counts.

It is computationally prohibitive because our vocabulary size is often larger than 50k, making \\(M\\) a beast for SVD algorithms.

---

### Predictive: the new way

In 2013 Tomas Mikolov published two papers which offered new ways to compute  word vectors. The first one, {% cite mikolov2013efficient %}, introduced CBOW and SkipGram, two different algorithm both now known as Word2Vec. The second, {% cite mikolov2013distributed %}, showed how to use previously introduced techniques such as hierarchical softmax and noise contrastive estimation

He released his [code](https://github.com/tmikolov/word2vec).  
While count-based methods would mean days or weeks of computation, Mikolov's ran in a few hours, with word embeddings of great _quality_ (a vague notion that I will talk about in a post to come).

His method and those that followed are called **predictive**. Some affiliate them with Deep Learning, which it is not since there's barely one hidden layer in the network.

Mikolov's two methods are called Continuous Bag Of Words (CBOW) and Skip-Gram (SG), they are called predictive because:

* CBOW tries to guess a word at the center of a window: `the cat .. the mat`,
* SG does the opposite, it tries to guess the window for a given word: `.. .. on .. ..`.

He built on two papers presenting new techniques for neural networks: __hierarchical softmax__, a complex method to compute softmaxs in \\(\mathcal{O}(\log n)\\) time, and __noise contrastive estimation__, _a.k.a._ negative sampling.

These two notions are well explained by S. Ruder on his [blog](http://sebastianruder.com/word-embeddings-softmax/index.html), so let's review the basics.

---

Cited references

{% bibliography --cited %}
