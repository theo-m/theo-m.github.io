---
layout: main
title: Word vectors 101
date: 2017-10-11
---


If the field of natural language processings (NLP) was a kitchen, word embeddings would be its potatoes. They are everywhere, and used everywhere to cook finer meals (i.e. language models).

## What are they?

They are essentially word represented as vectors. So if you have a vocabulary \\(V = \\{ \text{animal, anarchy, }\dots \\} \\) you will assign each of its words to a real valued vector of dimension \\(d\\), e.g.: 

$$ v_{\text{animal}} = (0.77,~\dots,~-0.65)\in \mathbb R^d $$

The choice of \\(d\\) is task dependent: if you're going to use the vectors to perform classification tasks like named entity recognition (NER), you'll have better results with \\(d \sim 300\\), on the other hand, for translation tasks, we use bigger spaces with \\(d \sim 1000\\).

## How do we generate them?

There are several methods available, we're going to see 3 in details:

* PMI based methods
* Word2Vec
* GloVe

These methods are canonical and "general purpose". They all use the distributional semantics hypothesis: word meanings are defined by words they cooccur with. A good quote to keep in mind:

> _A word is characterized by the company it keeps._  
{% cite firth1957synopsis %}

This means that we have two new parameters: first the corpus from which we will gather this cooccurence information, and second the definition of "cooccuring with". Usually, the corpus is wikipedia dumps and cooccurence defined as "word1 occuring within \\(k\\) words from word2".

A third parameter is whether to keep all unique words from the corpus or get rid of the rare ones. If the latter is chosen (usual case), one needs to set a minimum number of apparition for words to get in the vocabulary.


#### PMI based methods

They are what {% cite baroni2014don %} describe as "count-based" methods because they directly use cooccurence counts in a given corpus. In fact they keep track of a subtler measure of cooccurence: pointwise mutual information (PMI):

$$ \begin{align} \text{PMI}(\text{animal, umbrella}) & = \log \frac{\mathbb P[\text{animal, umbrella}]}{\mathbb P [\text{animal}] \cdot \mathbb P [\text{umbrella}]} \\ & = \log \frac{\#(w1, w2) \cdot \text{corpus size} }{\#(w1) \cdot \#(w2)} \end{align} $$

We store all these PMI pairs for all words from the vocabulary in a _big_ matrix

$$ M = \left[ \begin{matrix} 0.06 & \cdots & 1.12 \\ \vdots & \ddots & \vdots \\ 1.12 & \cdots & 0.01 \\ \end{matrix}\right] \in \mathbb R^{|V| \times |V|} $$

This matrix is _big_ since \\( \|V\| \\), the size of the vocabulary, is in the hundred of thousands. But thankfully it is also symmetric positive, which authorises singular value decomposition (SVD).

Each row, or column, of this matrix represents a given word in the vocabulary, so once the SVD is performed we can order its biggest singular value and truncate the vectors to the desired dimension \\(d\\), which gives us our word embedding.

The disadvantage of this method is the computational cost implied by the use of the SVD; it takes weeks of computation to get word embeddings from even medium sized corpora.

#### Word2Vec

In 2013 Tomas Mikolov released two papers, {% cite mikolov2013distributed %} and {% cite mikolov2013efficient %} that introduced two algorithms: Continuous Bag of Words (CBOW) and Skip-Gram (SG), and two adaptations of computational tricks: hierarchical softmax (H-Softmax) and negative sampling (NS).

For SGNS, the setting is the following: initialize with the uniform distribution over \\([-d^{-1/2}, d^{-1/2}]\\) a matrix \\( M \in \mathbb R^{\|V\| \times d}\\) and a matrix \\(M'\\) of same size with zeroes. The first is your embedding matrix, with the rows as the word vectors of dimension \\(d\\), the second the context matrix.

While going through the corpus word by word, pick the current word's vector and compute its dot product with its context words:

$$ \dots \text{the leopard is an ANIMAL living in the jungle} \dots $$

$$ (M_{i_{\text{animal}}j})_{1<j<d} = v_{\text{animal}} $$

$$ v_{\text{animal}} \cdot v_{w} ~~ \text{for } w \in \{\text{the, leopard, ..., the, jungle} \}$$

And pick a certain number of false word context pairs to learn to discriminate them from the previous pairs.

---

Cited references

{% bibliography --cited %}